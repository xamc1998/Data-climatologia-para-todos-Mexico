{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44c6090",
   "metadata": {},
   "source": [
    "Procesamiento de Datos y Consolidación de la Base de Datos Climática\n",
    "Una vez obtenidos los enlaces directos a los archivos históricos, este notebook ejecuta la fase crítica de ETL (Extracción, Transformación y Carga). El desafío principal aquí radica en la heterogeneidad y falta de estructura de los archivos .txt de CONAGUA, los cuales están diseñados para lectura humana y no para procesamiento computacional.\n",
    "\n",
    "La estrategia de consolidación se basa en los siguientes pilares técnicos:\n",
    "\n",
    "1. Extracción Masiva Multihilo\n",
    "Dado que el proyecto gestiona más de 5,300 estaciones, realizar peticiones secuenciales sería ineficiente. El script implementa un flujo de procesamiento masivo que:\n",
    "\n",
    "- Realiza peticiones HTTP a cada URL obtenida en la fase anterior.\n",
    "\n",
    "- Valida la disponibilidad del archivo antes de intentar el parseo.\n",
    "\n",
    "- Gestiona errores de conexión para asegurar que una URL caída no interrumpa el flujo total.\n",
    "\n",
    "2. Ingeniería de Parseo de Texto Plano\n",
    "Los archivos .txt contienen tablas climatológicas con formatos variables. Para estandarizar esta información, el código realiza las siguientes operaciones:\n",
    "\n",
    "- Identificación de Patrones: Localiza las secciones de interés (Precipitación Total o Temperatura Media) dentro del cuerpo del texto.\n",
    "\n",
    "- Limpieza de Ruido: Elimina encabezados, pies de página y caracteres especiales que no aportan valor numérico.\n",
    "\n",
    "- Manejo de Valores Nulos: Detecta y estandariza los códigos de \"datos faltantes\" propios de las estaciones meteorológicas para evitar sesgos en el análisis posterior.\n",
    "\n",
    "3. Estructuración y Normalización de la Base de Datos\n",
    "- El resultado final es la creación de un DataFrame maestro con las siguientes características:\n",
    "\n",
    "- Indexación por Estación y Tiempo: Cada registro se vincula inequívocamente a su ID de estación (Estacion/id) y su correspondiente año.\n",
    "\n",
    "- Transformación de Columnas (Pivotado): Los datos mensuales se organizan en columnas estandarizadas (Enero a Diciembre), facilitando el cálculo de promedios anuales y tendencias estacionales.\n",
    "\n",
    "- Tipado de Datos: Se asegura que todas las métricas climáticas sean de tipo flotante (float) para permitir operaciones aritméticas inmediatas.\n",
    "\n",
    "4. Exportación y Persistencia\n",
    "Para garantizar la portabilidad de la información, el notebook culmina con la exportación de los datos consolidados a formato CSV.\n",
    "\n",
    "- Encoding UTF-8: Se utiliza codificación estándar para preservar la integridad de los nombres de estaciones con caracteres especiales o acentos.\n",
    "\n",
    "- Auditoría de Carga: El sistema imprime un resumen final indicando el número total de filas procesadas y los tipos de datos asignados, sirviendo como un check de calidad de la base de datos resultante.\n",
    "\n",
    "- Impacto del Proyecto: Esta base de datos consolidada reduce semanas de trabajo manual a solo minutos de ejecución automatizada, permitiendo que investigadores y analistas se enfoquen en la interpretación del clima en lugar de la limpieza de archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d979d",
   "metadata": {},
   "source": [
    "- Ejemplo visual de la informacion sin estructurar\n",
    "\n",
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c880cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraccion masiva de LLUVIA TOTAL MENSUAL desde URLs txt de CONAGUA\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "# Ruta donde se encuentra el listado de URLs\n",
    "RUTA_ARCHIVO_URLS = r\"resultados\\URL - Listado limpio.csv\"\n",
    "COLUMNA_URL = \"URL_Mensual\" \n",
    "\n",
    "\n",
    "# --- FUNCIÓN DE EXTRACCIÓN POR URL (CON CONVERSIÓN DE CLAVES A STRING) ---\n",
    "\n",
    "def crear_df_lluvia_mensual(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Descarga el archivo TXT de la CONAGUA y procesa EXCLUSIVAMENTE\n",
    "    la sección 'LLUVIA TOTAL MENSUAL'. Convierte las claves 'AÑO' y \n",
    "    'Estacion/id' a string.\n",
    "    \"\"\"\n",
    "    SECCION_CLAVE = \"LLUVIA TOTAL MENSUAL\"\n",
    "    \n",
    "    # 1. Descargar el contenido del archivo\n",
    "    try:\n",
    "        user_agents = [\n",
    "         \"\"\"\n",
    "           Se recomienta el uso de Mozila\n",
    "                   \"\"\"\n",
    "        ]\n",
    "        headers = {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=30) \n",
    "        response.raise_for_status() \n",
    "        contenido = response.text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Extraer Metadatos (Encabezado)\n",
    "    patrones = {\n",
    "        'Estacion/id': r\"ESTACIÓN\\s*:\\s*(\\d+)\",\n",
    "        'Nombre': r\"NOMBRE\\s*:\\s*([^\\n]+)\",\n",
    "        'Estado': r\"ESTADO\\s*:\\s*([^\\n]+)\",\n",
    "        'Municipio': r\"MUNICIPIO\\s*:\\s*([^\\n]+)\",\n",
    "        'Situación': r\"SITUACIÓN\\s*:\\s*([^\\n]+)\",\n",
    "        'Latitude': r\"LATITUD\\s*:\\s*([\\d\\.-]+\\s*°)\",\n",
    "        'Longitude': r\"LONGITUD\\s*:\\s*([\\d\\.-]+\\s*°)\",\n",
    "    }\n",
    "    \n",
    "    metadata = {}\n",
    "    for key, pattern in patrones.items():\n",
    "        match = re.search(pattern, contenido)\n",
    "        if match:\n",
    "            value = match.group(1).strip().replace(' °', '').replace('°', '')\n",
    "            metadata[key] = value\n",
    "\n",
    "    # 3. AISLAR Y LIMPIAR SOLO la SECCION_CLAVE\n",
    "    inicio_seccion = contenido.find(SECCION_CLAVE)\n",
    "    \n",
    "    if inicio_seccion == -1:\n",
    "        # MANEJO DE ERROR: Advertir si la sección no existe\n",
    "        estacion_id = metadata.get('Estacion/id', 'N/A')\n",
    "        print(f\"   [ADVERTENCIA] Sección '{SECCION_CLAVE}' NO ENCONTRADA en URL: {url}. Estación ID: {estacion_id}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    seccion_completa = contenido[inicio_seccion:]\n",
    "    datos_tabla_limpio = []\n",
    "    capturar_datos = False\n",
    "    \n",
    "    for linea in seccion_completa.splitlines():\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        if SECCION_CLAVE in linea:\n",
    "            capturar_datos = True \n",
    "            continue\n",
    "        \n",
    "        # Detener al encontrar filas estadísticas (FIN DE LA TABLA)\n",
    "        if \"MÍNIMA\" in linea or \"MÁXIMA\" in linea or \"MEDIA\" in linea or \"DESV.ST\" in linea:\n",
    "            break\n",
    "            \n",
    "        if capturar_datos and linea:\n",
    "            datos_tabla_limpio.append(linea)\n",
    "\n",
    "    # 4. Cargar los datos en un DataFrame\n",
    "    datos_df = \"\\n\".join(datos_tabla_limpio)\n",
    "    \n",
    "    if not datos_df.strip():\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    try:\n",
    "        # Lectura de la tabla (TSV)\n",
    "        df = pd.read_csv(StringIO(datos_df), sep='\\t', skipinitialspace=True, na_values=['', ' '], engine='python')\n",
    "    except Exception:\n",
    "        # Falla al convertir la tabla a DataFrame\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # 5. Aplicar la limpieza y la estructura final solicitada\n",
    "    columnas_renombrar = {\n",
    "        'ACUM': 'Acum Σ', 'PROM': 'Promedio', 'MESES': 'Meses',\n",
    "        'ENE': 'Enero', 'FEB': 'Febrero', 'MAR': 'Marzo', 'ABR': 'Abril', 'MAY': 'Mayo',\n",
    "        'JUN': 'Junio', 'JUL': 'Julio', 'AGO': 'Agosto', 'SEP': 'Septiembre', \n",
    "        'OCT': 'Octubre', 'NOV': 'Noviembre', 'DIC': 'Diciembre' \n",
    "    }\n",
    "    df.rename(columns=columnas_renombrar, inplace=True)\n",
    "\n",
    "    # Convertir AÑO a string antes de insertar metadatos\n",
    "    if 'AÑO' in df.columns:\n",
    "        df['AÑO'] = df['AÑO'].astype(str).str.strip()\n",
    "        \n",
    "    # Insertar columnas de metadatos y reordenar\n",
    "    for i, (key, value) in enumerate(metadata.items()):\n",
    "        # CONVERSIÓN CLAVE: Insertar metadatos como string\n",
    "        df.insert(loc=i, column=key, value=str(value)) \n",
    "        \n",
    "    # CONVERSIÓN CLAVE FINAL: Asegurar que 'Estacion/id' (metadato) sea string\n",
    "    if 'Estacion/id' in df.columns:\n",
    "        df['Estacion/id'] = df['Estacion/id'].astype(str).str.strip()\n",
    "\n",
    "\n",
    "    columnas_meses_completos = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', \n",
    "                                'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', \n",
    "                                'Diciembre']\n",
    "                      \n",
    "    columnas_finales = list(metadata.keys()) + ['AÑO'] + columnas_meses_completos + ['Acum Σ', 'Promedio', 'Meses']\n",
    "    df = df.reindex(columns=columnas_finales)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- FUNCIÓN DE PROCESAMIENTO MASIVO COMPLETO ---\n",
    "\n",
    "def procesar_extraccion_masiva(ruta_csv_urls: str, columna_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Coordina la descarga y procesamiento de TODAS las URLs, consolidando los resultados.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ruta_csv_urls):\n",
    "        print(f\"Error de archivo: No se encontró el archivo de URLs en la ruta: {ruta_csv_urls}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df_urls = pd.read_csv(ruta_csv_urls, usecols=[columna_url])\n",
    "        urls_a_procesar = df_urls[columna_url].dropna().unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar URLs desde el CSV: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not urls_a_procesar:\n",
    "        print(\"Advertencia: No se encontraron URLs válidas para procesar.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_urls = len(urls_a_procesar)\n",
    "    print(f\"\\nIniciando extracción masiva completa de {total_urls} URLs para LLUVIA TOTAL MENSUAL...\")\n",
    "    \n",
    "    lista_df_estaciones = []\n",
    "    \n",
    "    for i, url in enumerate(urls_a_procesar):\n",
    "        print(f\"-> Procesando {i+1} de {total_urls}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            df_estacion = crear_df_lluvia_mensual(url)\n",
    "            \n",
    "            if not df_estacion.empty:\n",
    "                lista_df_estaciones.append(df_estacion)\n",
    "                print(f\"   [ÉXITO] Estación procesada.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   [ERROR INESPERADO] Falló el procesamiento interno de {url}: {e}\")\n",
    "            \n",
    "        # Pausa aleatoria para no saturar el servidor\n",
    "        time.sleep(random.uniform(1, 3)) \n",
    "            \n",
    "    if not lista_df_estaciones:\n",
    "        print(\"\\n¡PROCESO FINALIZADO! No se pudieron extraer datos válidos para consolidar.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(f\"\\nConsolidando {len(lista_df_estaciones)} DataFrames...\")\n",
    "    df_consolidado = pd.concat(lista_df_estaciones, ignore_index=True)\n",
    "    \n",
    "    print(f\"Total de filas consolidadas en la variable: {df_consolidado.shape[0]}\")\n",
    "    return df_consolidado\n",
    "\n",
    "# --- BLOQUE DE EJECUCIÓN ---\n",
    "\n",
    "# 1. Ejecutar la extracción masiva completa\n",
    "df_base_datos_lluvia = procesar_extraccion_masiva(\n",
    "    ruta_csv_urls=RUTA_ARCHIVO_URLS, \n",
    "    columna_url=COLUMNA_URL\n",
    ")\n",
    "\n",
    "print(\"\\n--- PROCESO DE EXTRACCIÓN MASIVA FINALIZADO ---\")\n",
    "\n",
    "# 2. El DataFrame consolidado se encuentra en la variable 'df_base_datos_lluvia'\n",
    "if not df_base_datos_lluvia.empty:\n",
    "    print(f\"\\nBase de datos consolidada (LLUVIA TOTAL MENSUAL) creada en la variable 'df_base_datos_lluvia'.\")\n",
    "    print(f\"Tipos de dato de las columnas clave: 'Estacion/id' ({df_base_datos_lluvia['Estacion/id'].dtype}) y 'AÑO' ({df_base_datos_lluvia['AÑO'].dtype})\")\n",
    "    print(f\"Primeras 5 filas del resultado (Total de Filas: {df_base_datos_lluvia.shape[0]}):\")\n",
    "    print(df_base_datos_lluvia.head())\n",
    "else:\n",
    "    print(\"\\nLa variable 'df_base_datos_lluvia' está vacía. No se pudo extraer información válida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283fae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_datos_lluvia.to_csv(r\"resultados\\Base de datos Lluvia.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraccion masiva de TEMPERATURA MEDIA MENSUAL desde URLs txt de CONAGUA\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "# Ruta donde se encuentra el listado de URLs (misma que la usada para la lluvia)\n",
    "RUTA_ARCHIVO_URLS = r\"resultados\\RL - Listado limpio.csv\"\n",
    "COLUMNA_URL = \"URL_Mensual\" \n",
    "\n",
    "\n",
    "# --- FUNCIÓN DE EXTRACCIÓN POR URL (TEMPERATURA) ---\n",
    "\n",
    "def crear_df_temperatura_mensual(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Descarga el archivo TXT de la CONAGUA, extrae metadatos y procesa \n",
    "    EXCLUSIVAMENTE la sección 'TEMPERATURA MEDIA MENSUAL'.\n",
    "    Convierte las claves 'AÑO' y 'Estacion/id' a string.\n",
    "    \"\"\"\n",
    "    SECCION_CLAVE = \"TEMPERATURA MEDIA MENSUAL\"\n",
    "    \n",
    "    # 1. Descargar el contenido del archivo\n",
    "    try:\n",
    "        # Usamos un User-Agent y un timeout para una descarga más robusta\n",
    "        user_agents = [\n",
    "           \"se recomienda el uso de \" \n",
    "        ]\n",
    "        headers = {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=30) \n",
    "        response.raise_for_status() \n",
    "        contenido = response.text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Extraer Metadatos (Encabezado)\n",
    "    patrones = {\n",
    "        'Estacion/id': r\"ESTACIÓN\\s*:\\s*(\\d+)\",\n",
    "        'Nombre': r\"NOMBRE\\s*:\\s*([^\\n]+)\",\n",
    "        'Estado': r\"ESTADO\\s*:\\s*([^\\n]+)\",\n",
    "        'Municipio': r\"MUNICIPIO\\s*:\\s*([^\\n]+)\",\n",
    "        'Situación': r\"SITUACIÓN\\s*:\\s*([^\\n]+)\",\n",
    "        'Latitude': r\"LATITUD\\s*:\\s*([\\d\\.-]+\\s*°)\",\n",
    "        'Longitude': r\"LONGITUD\\s*:\\s*([\\d\\.-]+\\s*°)\",\n",
    "    }\n",
    "    \n",
    "    metadata = {}\n",
    "    for key, pattern in patrones.items():\n",
    "        match = re.search(pattern, contenido)\n",
    "        if match:\n",
    "            value = match.group(1).strip().replace(' °', '').replace('°', '')\n",
    "            metadata[key] = value\n",
    "\n",
    "    # 3. AISLAR Y LIMPIAR SOLO la SECCION_CLAVE\n",
    "    inicio_seccion = contenido.find(SECCION_CLAVE)\n",
    "    \n",
    "    if inicio_seccion == -1:\n",
    "        # MANEJO DE ERROR SOLICITADO: Advertir si la sección no existe\n",
    "        estacion_id = metadata.get('Estacion/id', 'N/A')\n",
    "        print(f\"   [ADVERTENCIA] Sección '{SECCION_CLAVE}' NO ENCONTRADA en URL: {url}. Estación ID: {estacion_id}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    seccion_completa = contenido[inicio_seccion:]\n",
    "    datos_tabla_limpio = []\n",
    "    capturar_datos = False\n",
    "    \n",
    "    for linea in seccion_completa.splitlines():\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        if SECCION_CLAVE in linea:\n",
    "            capturar_datos = True \n",
    "            continue\n",
    "        \n",
    "        # Detener al encontrar filas estadísticas (FIN DE LA TABLA)\n",
    "        if \"MÍNIMA\" in linea or \"MÁXIMA\" in linea or \"MEDIA\" in linea or \"DESV.ST\" in linea:\n",
    "            break\n",
    "            \n",
    "        if capturar_datos and linea:\n",
    "            datos_tabla_limpio.append(linea)\n",
    "\n",
    "    # 4. Cargar los datos en un DataFrame\n",
    "    datos_df = \"\\n\".join(datos_tabla_limpio)\n",
    "    \n",
    "    if not datos_df.strip():\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(StringIO(datos_df), sep='\\t', skipinitialspace=True, na_values=['', ' '], engine='python')\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # 5. Aplicar la limpieza, estructura y tipado\n",
    "    columnas_renombrar = {\n",
    "        # Mantengo 'Acum Σ' por consistencia con la estructura de la función original,\n",
    "        # aunque en temperatura a menudo es irrelevante o se usa para Suma de T\n",
    "        'ACUM': 'Acum T', \n",
    "        'PROM': 'Promedio',\n",
    "        'MESES': 'Meses',\n",
    "        'ENE': 'Enero', 'FEB': 'Febrero', 'MAR': 'Marzo', 'ABR': 'Abril', 'MAY': 'Mayo',\n",
    "        'JUN': 'Junio', 'JUL': 'Julio', 'AGO': 'Agosto', 'SEP': 'Septiembre', \n",
    "        'OCT': 'Octubre', 'NOV': 'Noviembre', 'DIC': 'Diciembre' \n",
    "    }\n",
    "    df.rename(columns=columnas_renombrar, inplace=True)\n",
    "\n",
    "    # CONVERSIÓN A STRING: AÑO\n",
    "    if 'AÑO' in df.columns:\n",
    "        df['AÑO'] = df['AÑO'].astype(str).str.strip()\n",
    "        \n",
    "    # Insertar columnas de metadatos (claves se insertan como string)\n",
    "    for i, (key, value) in enumerate(metadata.items()):\n",
    "        df.insert(loc=i, column=key, value=str(value)) \n",
    "        \n",
    "    # CONVERSIÓN A STRING: Estacion/id\n",
    "    if 'Estacion/id' in df.columns:\n",
    "        df['Estacion/id'] = df['Estacion/id'].astype(str).str.strip()\n",
    "\n",
    "\n",
    "    columnas_meses_completos = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', \n",
    "                                'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', \n",
    "                                'Diciembre']\n",
    "                      \n",
    "    columnas_finales = list(metadata.keys()) + ['AÑO'] + columnas_meses_completos + ['Acum T', 'Promedio', 'Meses']\n",
    "    df = df.reindex(columns=columnas_finales)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- FUNCIÓN DE PROCESAMIENTO MASIVO (TEMPERATURA) ---\n",
    "\n",
    "def procesar_extraccion_masiva_temperatura(ruta_csv_urls: str, columna_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Coordina la descarga y procesamiento de TODAS las URLs para Temperatura Media Mensual.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ruta_csv_urls):\n",
    "        print(f\"Error de archivo: No se encontró el archivo de URLs en la ruta: {ruta_csv_urls}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df_urls = pd.read_csv(ruta_csv_urls, usecols=[columna_url])\n",
    "        urls_a_procesar = df_urls[columna_url].dropna().unique().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar URLs desde el CSV: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not urls_a_procesar:\n",
    "        print(\"Advertencia: No se encontraron URLs válidas para procesar.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_urls = len(urls_a_procesar)\n",
    "    print(f\"\\nIniciando extracción masiva completa de {total_urls} URLs para TEMPERATURA MEDIA MENSUAL...\")\n",
    "    \n",
    "    lista_df_estaciones = []\n",
    "    \n",
    "    for i, url in enumerate(urls_a_procesar):\n",
    "        print(f\"-> Procesando {i+1} de {total_urls}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # Llama a la función específica de temperatura\n",
    "            df_estacion = crear_df_temperatura_mensual(url)\n",
    "            \n",
    "            if not df_estacion.empty:\n",
    "                lista_df_estaciones.append(df_estacion)\n",
    "                print(f\"   [ÉXITO] Estación procesada.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   [ERROR INESPERADO] Falló el procesamiento interno de {url}: {e}\")\n",
    "            \n",
    "        # Pausa aleatoria\n",
    "        time.sleep(random.uniform(1, 3)) \n",
    "            \n",
    "    if not lista_df_estaciones:\n",
    "        print(\"\\n¡PROCESO FINALIZADO! No se pudieron extraer datos válidos para consolidar.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(f\"\\nConsolidando {len(lista_df_estaciones)} DataFrames...\")\n",
    "    df_consolidado = pd.concat(lista_df_estaciones, ignore_index=True)\n",
    "    \n",
    "    print(f\"Total de filas consolidadas en la variable: {df_consolidado.shape[0]}\")\n",
    "    return df_consolidado\n",
    "\n",
    "# --- BLOQUE DE EJECUCIÓN ---\n",
    "\n",
    "# 1. Ejecutar la extracción masiva completa para TEMPERATURA\n",
    "df_base_datos_temperatura = procesar_extraccion_masiva_temperatura(\n",
    "    ruta_csv_urls=RUTA_ARCHIVO_URLS, \n",
    "    columna_url=COLUMNA_URL\n",
    ")\n",
    "\n",
    "print(\"\\n--- PROCESO DE EXTRACCIÓN DE TEMPERATURA FINALIZADO ---\")\n",
    "\n",
    "# 2. El DataFrame consolidado se encuentra en la variable 'df_base_datos_temperatura'\n",
    "if not df_base_datos_temperatura.empty:\n",
    "    print(f\"\\n✅ Base de datos consolidada (TEMPERATURA MEDIA MENSUAL) creada en la variable 'df_base_datos_temperatura'.\")\n",
    "    print(f\"Tipos de dato de las columnas clave: 'Estacion/id' ({df_base_datos_temperatura['Estacion/id'].dtype}) y 'AÑO' ({df_base_datos_temperatura['AÑO'].dtype})\")\n",
    "    print(f\"Primeras 5 filas del resultado (Total de Filas: {df_base_datos_temperatura.shape[0]}):\")\n",
    "    print(df_base_datos_temperatura.head())\n",
    "else:\n",
    "    print(\"\\nLa variable 'df_base_datos_temperatura' está vacía. No se pudo extraer información válida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbe72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_datos_temperatura.to_csv(r\"resultados\\Base de datos temperatura.csv\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
